# example configuration file for ceph-bluestore.fio

[global]
	debug bluestore = 0/0
	debug bluefs = 0/0
	debug bdev = 0/0
	debug rocksdb = 0/0
	# spread objects over 8 collections
	osd pool default pg num = 8
	# increasing shards can help when scaling number of collections
	osd op num shards = 5

[osd]
	osd objectstore = bluestore

	# use directory= option from fio job file
	osd data = ${fio_dir}

	# log inside fio_dir
	log file = ${fio_dir}/log
