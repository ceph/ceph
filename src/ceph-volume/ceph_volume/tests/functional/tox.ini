[tox]
envlist = centos-bluestore-{lvm,raw}-{unencrypted,dmcrypt}
skipsdist = True

[testenv]
deps =
  mock
  ansible
  pytest
  testinfra
allowlist_externals =
    vagrant
    bash
    git
    cp
    sleep
passenv=*
setenv=
  ANSIBLE_SSH_ARGS = -F {changedir}/vagrant_ssh_config -o ControlMaster=auto -o ControlPersist=600s -o PreferredAuthentications=publickey
  ANSIBLE_STDOUT_CALLBACK = debug
  VAGRANT_CWD = {changedir}
  CEPH_VOLUME_DEBUG = 1
  DEBIAN_FRONTEND=noninteractive
  ANSIBLE_COLLECTIONS_PATH = {envdir}/ansible_collections
  CONTAINER_IMAGE_REGISTRY = {env:CEPH_CONTAINER_IMAGE_REGISTRY:quay.ceph.io}
  CONTAINER_IMAGE_NAME = {env:CEPH_CONTAINER_IMAGE_NAME:ceph-ci/ceph}
  CONTAINER_IMAGE_TAG = {env:CEPH_CONTAINER_IMAGE_TAG:main}
  auto: BATCH_BEHAVIOR = --auto
  no_auto: BATCH_BEHAVIOR = --no-auto
  unencrypted: DMCRYPT =
  dmcrypt: DMCRYPT = --dmcrypt
  ; mixed_type: DEVICES_SETUP_PLAYBOOK = setup-nvme-devices.yml
  ; single_type: DEVICES_SETUP_PLAYBOOK = setup-hdd-devices.yml
changedir=
  centos-bluestore-lvm-unencrypted: {toxinidir}/centos/bluestore/lvm/unencrypted
  centos-bluestore-lvm-dmcrypt: {toxinidir}/centos/bluestore/lvm/dmcrypt
  centos-bluestore-raw-unencrypted: {toxinidir}/centos/bluestore/raw/unencrypted
  centos-bluestore-raw-dmcrypt: {toxinidir}/centos/bluestore/raw/dmcrypt
commands=
  bash {toxinidir}/scripts/vagrant_up.sh {posargs:--provider=virtualbox}
  bash {toxinidir}/scripts/generate_ssh_config.sh {changedir}

## clone cephadm-ansible
##  git clone -b {env:CEPHADM_ANSIBLE_BRANCH:devel} --depth 1 --single-branch {env:CEPHADM_ANSIBLE_REPO:"https://github.com/ceph/cephadm-ansible.git"} {envdir}/tmp/cephadm-ansible
## TODO: maybe replace this with cepahdm-preflight.yml
## bootstrap a minimal cluster
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/playbooks/bootstrap-cluster.yml -e 'ghprbPullId={env:ghprbPullId:} ceph_dev_branch={env:ghprbTargetBranch:main} ceph_dev_sha1={env:CEPH_DEV_SHA1:latest} ceph_container_image_registry={env:CONTAINER_IMAGE_REGISTRY} ceph_container_image_name={env:CONTAINER_IMAGE_NAME} ceph_container_image_tag={env:CONTAINER_IMAGE_TAG}'

  # test cluster state using testinfra
  py.test --sudo -v --connection=ansible --ssh-config={changedir}/vagrant_ssh_config --ansible-inventory={changedir}/hosts --hosts='ansible://osd0' {toxinidir}/tests

#  # reboot all vms - attempt
#  bash {toxinidir}/../scripts/vagrant_reload.sh {env:VAGRANT_UP_FLAGS:"--no-provision"} {posargs:--provider=virtualbox}
#
#  # after a reboot, osds may take about 20 seconds to come back up
#  sleep 30
#
#  # retest to ensure cluster came back up correctly after rebooting
#  py.test --reruns 5 --reruns-delay 10 -n 4 --sudo -v --connection=ansible --ssh-config={changedir}/vagrant_ssh_config --ansible-inventory={changedir}/hosts {toxinidir}/../tests
#
  # destroy an OSD, zap its device and recreate it using its ID
 ansible-playbook -vv -i {changedir}/hosts {toxinidir}/playbooks/test.yml
#
#  # retest to ensure cluster came back up correctly
  py.test --sudo -v --connection=ansible --ssh-config={changedir}/vagrant_ssh_config --ansible-inventory={changedir}/hosts --hosts='ansible://osd0' {toxinidir}/tests

  # test zap OSDs by ID
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/playbooks/test-zap.yml

  # test OSDs are well re-created
  py.test --sudo -v --connection=ansible --ssh-config={changedir}/vagrant_ssh_config --ansible-inventory={changedir}/hosts --hosts='ansible://osd0' {toxinidir}/tests

  vagrant destroy {env:VAGRANT_DESTROY_FLAGS:"--force"}
