# 1. 检查集群
## 1.1 检查集群的状态
```
#主要检查集群状态是否是 HEALTH_OK
 
ceph -s
    cluster aa7e0345-87a9-4860-a6b5-3158fd00b5a9
     health HEALTH_WARN
            nodeep-scrub,sortbitwise flag(s) set
     monmap e3: 3 mons at {op-xxx-ceph00=10.69.1.1:6789/0,op-xxx-ceph01=10.69.1.2:6789/0,op-xxx-ceph02=10.69.1.3:6789/0}
            election epoch 264, quorum 0,1,2 op-xxx-ceph02,op-xxx-ceph00,op-xxx-ceph01
      fsmap e12557: 1/1/1 up {0=op-xxx-ceph05=up:active}, 1 up:standby
     osdmap e24858: 59 osds: 59 up, 59 in
            flags nodeep-scrub,sortbitwise
      pgmap v50468419: 28832 pgs, 14 pools, 34479 GB data, 41212 kobjects
            101 TB used, 112 TB / 214 TB avail
               28822 active+clean
                  10 active+clean+scrubbing
  client io 329 kB/s wr, 0 op/s rd, 51 op/s wr
```
**输出信息里包含：**
 - 集群的 ID
 - 集群健康状况
 - monitor map 版本和 mon 法定人数状态
 - OSD map 版本和 OSD 状态摘要
 - PG map 版本
 - PG 和 Pool 的数量
 - 集群存储的数据量，对象的总量，以及集群的已用容量/总容量/可用 
 - 容量
 - 客户端的 iops 信息

## 1.2 检查集群的容量情况
```
#查看集群容量使用情况 %RAW USED
 
ceph df
GLOBAL:
    SIZE     AVAIL     RAW USED     %RAW USED
    214T      112T         101T         47.45
POOLS:
    NAME                          ID     USED       %USED     MAX AVAIL     OBJECTS
    rbd                           0         115         0        31508G            4
    cephfs_data                   1      31252G     42.67        31508G     13821502
    cephfs_metadata               2      46967k         0        31508G        31385
    .rgw.root                     5        1636         0        31508G            4
    default.rgw.control           6           0         0        31508G            8
    default.rgw.data.root         7        6892         0        31508G           22
    default.rgw.gc                8           0         0        31508G           32
    default.rgw.log               9           0         0        31508G          127
    default.rgw.users.uid         10       1492         0        31508G            6
    default.rgw.users.swift       11         12         0        31508G            1
    default.rgw.users.keys        12         51         0        31508G            4
    default.rgw.meta              13      28985         0        31508G           87
    default.rgw.buckets.index     14          0         0        31508G           26
    default.rgw.buckets.data      15      3226G      4.41        31508G     28351099
```
**输出的 GLOBAL 段展示了数据所占用集群存储空间的概要：**
 - SIZE： 集群的总容量。
 - AVAIL： 集群的可用空间总量。
 - RAW USED：已用存储空间总量。
 - % RAW USED：已用存储空间比率。用此值对比 full ratio 和 near full ratio 来确保不会用尽集群空间。
输出的 POOLS 段展示了存储池列表及各存储池的大致使用率。本段没有反映出副本、克隆和快照的占用情况。例如，如果你把 1MB 的数据存储为对象，理论使用率将是 1MB ，但考虑到副本数、克隆数、和快照数，实际使用量可能是 2MB 或更多。
 - NAME：存储池名字。
 - ID：存储池唯一标识符。
 - USED：大概数据量，单位为 KB 、MB 或 GB ；
 - %USED：各存储池的大概使用率。
 - Objects：各存储池内的大概对象数。

# 2. 检查osd
## 2.1 检查osd状态
```
#所有OSD状态为up且in
ceph osd stat
     osdmap e24858: 59 osds: 59 up, 59 in
```
## 2.2 检查osd crushmap是否一致
```
#检查WEIGHT列，以及REWEIGHT 权重是否一致
 
ID WEIGHT    TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 218.21495 root default
-2  43.64299     host op-xxx-ceph00
 0   3.63699         osd.0                up  1.00000          1.00000
 1   3.63699         osd.1                up  1.00000          1.00000
 2   3.63699         osd.2                up  1.00000          1.00000
 3   3.63699         osd.3                up  1.00000          1.00000
 4   3.63699         osd.4                up  1.00000          1.00000
 5   3.63699         osd.5                up  1.00000          1.00000
 6   3.63699         osd.6                up  1.00000          1.00000
 7   3.63699         osd.7                up  1.00000          1.00000
 8   3.63699         osd.8                up  1.00000          1.00000
 9   3.63699         osd.9                up  1.00000          1.00000
10   3.63699         osd.10               up  1.00000          1.00000
11   3.63699         osd.11               up  1.00000          1.00000
-3  43.64299     host op-xxx-ceph01
13   3.63699         osd.13               up  1.00000          1.00000
14   3.63699         osd.14               up  1.00000          1.00000
15   3.63699         osd.15               up  1.00000          1.00000
16   3.63699         osd.16               up  1.00000          1.00000
17   3.63699         osd.17               up  1.00000          1.00000
18   3.63699         osd.18               up  1.00000          1.00000
19   3.63699         osd.19               up  1.00000          1.00000
20   3.63699         osd.20               up  1.00000          1.00000
21   3.63699         osd.21               up  1.00000          1.00000
22   3.63699         osd.22               up  1.00000          1.00000
23   3.63699         osd.23               up  1.00000          1.00000
24   3.63699         osd.24               up  1.00000          1.00000
```
## 2.3 检查osd容量是否均衡
```
#1. 检查%USE 查看每个osd容量情况是否一致
#2. 检查rack机架下面的host数量是否均衡
 
ceph osd df tree
ID   CLASS WEIGHT     REWEIGHT SIZE   USE    AVAIL  %USE VAR  PGS TYPE NAME
  -1       1394.89197        -  1394T 45797G  1350T 3.21 1.00   - root default
-118        348.72299        -   348T 11544G   337T 3.23 1.01   -     rack xxx-18
  -3         87.18100        - 89274G  2888G 86385G 3.24 1.01   -         host ceph-xxx-hdd00
   0   hdd    7.26500  1.00000  7439G   252G  7186G 3.40 1.06  81             osd.0
   1   hdd    7.26500  1.00000  7439G   235G  7203G 3.17 0.99  78             osd.1
   2   hdd    7.26500  1.00000  7439G   262G  7177G 3.52 1.10  94             osd.2
   3   hdd    7.26500  1.00000  7439G   284G  7155G 3.82 1.19 107             osd.3
   4   hdd    7.26500  1.00000  7439G   236G  7203G 3.18 0.99  85             osd.4
   5   hdd    7.26500  1.00000  7439G   245G  7193G 3.30 1.03  83             osd.5
   6   hdd    7.26500  1.00000  7439G   231G  7207G 3.11 0.97  91             osd.6
   7   hdd    7.26500  1.00000  7439G   257G  7181G 3.46 1.08 100             osd.7
   8   hdd    7.26500  1.00000  7439G   190G  7249G 2.56 0.80  75             osd.8
   9   hdd    7.26500  1.00000  7439G   212G  7227G 2.85 0.89  79             osd.9
  10   hdd    7.26500  1.00000  7439G   248G  7190G 3.34 1.04  88             osd.10
  11   hdd    7.26500  1.00000  7439G   231G  7207G 3.11 0.97 102             osd.11
-119        348.72299        -   348T 11196G   337T 3.14 0.98   -     rack xxx-19
 -11         87.18100        - 89274G  2830G 86443G 3.17 0.99   -         host ceph-xxx-hdd04
  48   hdd    7.26500  1.00000  7439G   223G  7215G 3.01 0.94  78             osd.48
  49   hdd    7.26500  1.00000  7439G   279G  7160G 3.75 1.17  99             osd.49
  50   hdd    7.26500  1.00000  7439G   198G  7241G 2.66 0.83  76             osd.50
  51   hdd    7.26500  1.00000  7439G   230G  7209G 3.09 0.97  87             osd.51
  52   hdd    7.26500  1.00000  7439G   257G  7181G 3.46 1.08  89             osd.52
  53   hdd    7.26500  1.00000  7439G   193G  7246G 2.60 0.81  76             osd.53
  54   hdd    7.26500  1.00000  7439G   222G  7216G 2.99 0.93  81             osd.54
  55   hdd    7.26500  1.00000  7439G   237G  7201G 3.20 1.00  90             osd.55
  56   hdd    7.26500  1.00000  7439G   234G  7205G 3.15 0.98  85             osd.56
  57   hdd    7.26500  1.00000  7439G   251G  7187G 3.38 1.05 104             osd.57
  58   hdd    7.26500  1.00000  7439G   244G  7195G 3.28 1.02  88             osd.58
  59   hdd    7.26500  1.00000  7439G   257G  7182G 3.46 1.08  98             osd.59
-120        348.72299        -   348T 11582G   337T 3.24 1.01   -     rack xxx-20
  -5         87.18100        - 89274G  3066G 86207G 3.43 1.07   -         host ceph-xxx-hdd01
  12   hdd    7.26500  1.00000  7439G   302G  7137G 4.07 1.27 104             osd.12
  13   hdd    7.26500  1.00000  7439G   241G  7197G 3.25 1.01 105             osd.13
  14   hdd    7.26500  1.00000  7439G   318G  7120G 4.29 1.34 123             osd.14
  15   hdd    7.26500  1.00000  7439G   229G  7210G 3.08 0.96  74             osd.15
  16   hdd    7.26500  1.00000  7439G   272G  7167G 3.66 1.14 101             osd.16
  17   hdd    7.26500  1.00000  7439G   264G  7174G 3.56 1.11  94             osd.17
  18   hdd    7.26500  1.00000  7439G   241G  7197G 3.25 1.01  93             osd.18
  19   hdd    7.26500  1.00000  7439G   245G  7193G 3.30 1.03  98             osd.19
  20   hdd    7.26500  1.00000  7439G   211G  7227G 2.84 0.89  74             osd.20
  21   hdd    7.26500  1.00000  7439G   264G  7174G 3.56 1.11  92             osd.21
  22   hdd    7.26500  1.00000  7439G   264G  7175G 3.55 1.11  93             osd.22
  23   hdd    7.26500  1.00000  7439G   208G  7230G 2.81 0.88  83             osd.23
-121        348.72299        -   348T 11474G   337T 3.21 1.00   -     rack xxx-21
  -7         87.18100        - 89274G  2844G 86429G 3.19 0.99   -         host ceph-xxx-hdd02
  24   hdd    7.26500  1.00000  7439G   212G  7226G 2.86 0.89  77             osd.24
  25   hdd    7.26500  1.00000  7439G   258G  7180G 3.48 1.08  85             osd.25
  26   hdd    7.26500  1.00000  7439G   223G  7216G 3.00 0.94  83             osd.26
  27   hdd    7.26500  1.00000  7439G   216G  7222G 2.91 0.91  86             osd.27
  28   hdd    7.26500  1.00000  7439G   252G  7187G 3.39 1.06  85             osd.28
  29   hdd    7.26500  1.00000  7439G   190G  7249G 2.56 0.80  82             osd.29
  30   hdd    7.26500  1.00000  7439G   255G  7183G 3.44 1.07  92             osd.30
  31   hdd    7.26500  1.00000  7439G   260G  7178G 3.51 1.09  90             osd.31
  32   hdd    7.26500  1.00000  7439G   241G  7198G 3.24 1.01  95             osd.32
  33   hdd    7.26500  1.00000  7439G   252G  7187G 3.39 1.06 102             osd.33
  34   hdd    7.26500  1.00000  7439G   279G  7160G 3.75 1.17  92             osd.34
  35   hdd    7.26500  1.00000  7439G   201G  7237G 2.71 0.85  79             osd.35
```
## 2.4 检查osd三副本是否分散机架
```
#抽查osd[31,38,1] 检查31，38，1是否分开多个rack，如果没有机架则分散多个host
 
ceph pg dump pgs|grep ^1|awk 'BEGIN {print "PG \t\t OBJECTS \t\t BYTES \t\tosd"} {print $1 "\t\t" $2 "\t\t" $7 "\t\t" $15}' | head
dumped pgs in format plain
PG       OBJECTS         BYTES      osd
1.f9f       3339        8198351373      [31,38,1]
1.f9e       3352        8060814080      [48,7,52]
1.f9d       3484        8460769474      [32,3,18]
1.f9c       3283        8113961689      [0,60,26]
1.f9b       3408        7863461676      [60,14,42]
1.f9a       3454        8449437644      [42,7,19]
1.f99       3326        8301474267      [8,18,39]
1.f98       3414        8259614607      [52,30,14]
1.f97       3373        8180890502      [2,19,52]
 
 
#三副本全量检查是否分配在多机架
master=""
slave1=""
slave2=""
ceph pg dump pgs|grep ^1|awk ' {print $15}' > pgs.log
sed -i "s/\[//g" pgs.log
sed -i "s/\]//g" pgs.log
for line in `cat pgs.log`
do
    master_num=`echo $line | cut -d "," -f 1`
    slave1_num=`echo $line | cut -d "," -f 2`
    slave2_num=`echo $line | cut -d "," -f 3`
 
    master=`ceph osd tree | grep  "osd.$master_num" -B 12 | grep host | tail -1 | awk -F  ' ' '{print $4}'`
    slave1=`ceph osd tree | grep  "osd.$slave1_num" -B 12 | grep host | tail -1 | awk -F  ' ' '{print $4}'`
    slave2=`ceph osd tree | grep  "osd.$slave2_num" -B 12 | grep host | tail -1 | awk -F  ' ' '{print $4}'`
    echo " master:" $master " slave1 " $slave1 " slave2 " $slave2
    master_ip=`host $master | awk -F ' ' '{print $4}' | cut -d "." -f 1,2`
    slave1_ip=`host $slave1 | awk -F ' ' '{print $4}' | cut -d "." -f 1,2`
    slave2_ip=`host $slave2 | awk -F ' ' '{print $4}' | cut -d "." -f 1,2`
    echo " master:" $master_ip " slave1 " $slave1_ip " slave2 " $slave2_ip
    if [[ "$master_ip" = "$slave1_ip" ]] || [[ "$slave1_ip" = "$slave2_ip" ]] || [[ "$master_ip" = "$slave2_ip" ]]; then
        echo "failed"
    break
    else
        echo "ok"
    fi
done
```
## 2.5 检查恢复限速配置
```
#检查恢复限速参数是否生效
#osd recovery max active = 3 （default : 15)
#osd recovery op priority = 3 (default : 10)
#osd max backfills = 1 (default : 10)
 
ceph daemon osd.6 config show | grep  -E 'osd_recovery_max_active|osd_recovery_op_priority|osd_max_backfills'
"osd_max_backfills": "1",
"osd_recovery_max_active": "1",
"osd_recovery_op_priority": "1",
```
## 2.6 检查数据清洗时间段
```
#检查恢复限速参数是否生效,凌晨0点-5点
#osd_scrub_begin_hour = 0
#osd_scrub_end_hour = 5
  
ceph daemon osd.6 config show | grep  -E 'osd_scrub_begin_hour|osd_scrub_end_hour'
"osd_scrub_begin_hour": "0",
"osd_scrub_end_hour": "5",
```
# 3. 检查mon
## 3.1 检查mon状态
```
#检查mon是否都是在线
 
ceph mon stat
e3: 3 mons at {op-xxx-ceph00=10.9.1.1:6789/0,op-xxx-ceph01=10.9.1.2:6789/0,op-xxx-ceph02=10.9.1.3:6789/0}, election epoch 264, quorum 0,1,2 op-xxx-ceph02,op-xxx-ceph00,op-xxx-ceph01
```
## 3.2 检查mon机架分布
```
#检查mon是否分布在多个机架
 
ceph mon dump
dumped monmap epoch 3
epoch 3
fsid aa7e0345-87a9-4860-a6b5-3158fd00b5a9
last_changed 2016-07-27 16:51:39.275244
created 2016-07-27 02:42:27.838682
0: 10.9.1.1:6789/0 mon.op-xxx-ceph02
1: 10.9.1.2:6789/0 mon.op-xxx-ceph00
2: 10.9.1.3:6789/0 mon.op-xxx-ceph01
```
# 4. 检查 MDS 
## 4.1 检查MDS状态
元数据服务器为 Ceph 文件系统提供元数据服务，不过在当前生产环境中并未部署 MDS 。

元数据服务器有两种状态： up | down 和 active | inactive ，执行下面的命令查看元数据服务器状态为 up 且 active ：
```
ceph mds stat
e12557: 1/1/1 up {0=op-xxx-ceph05=up:active}, 1 up:standby
```
# 5. 检查PG
## 5.1 检查PG状态
PG 把对象映射到 OSD 。监控 PG 时，我们希望它们的状态是 active 且 clean。
```
ceph pg stat
v50681978: 28832 pgs: 9 active+clean+scrubbing, 28823 active+clean; 34497 GB data, 101 TB used, 112 TB / 214 TB avail; 198 kB/s rd, 182 kB/s wr, 55 op/s
```

