os_type: ubuntu
os_version: "14.04"
overrides:
  ceph:
    conf:
      mon:
        debug mon: 20
        debug ms: 1
        debug paxos: 20
        mon warn on legacy crush tunables: false
        mon min osdmap epochs: 3
        osd pool default size: 2
        osd pool default min size: 1
      osd:
        osd map cache size: 2
        osd map max advance: 1
        debug filestore: 20
        debug journal: 20
        debug ms: 1
        debug osd: 20
    log-whitelist:
    - osd_map_cache_size
    - slow request
    - scrub mismatch
    - ScrubResult
    - failed to encode
    - wrongly marked me down
roles:
- - mon.a
  - osd.0
  - osd.1
  - mon.b
  - mon.c
  - osd.2
  - client.0
openstack:
  - volumes: # attached to each instance
      count: 3
      size: 10 # GB
tasks:
- install:
    branch: hammer
- print: '**** done installing hammer'
- ceph:
    fs: xfs
- print: '**** done ceph'
- ceph_manager.create_pool:
    args: ['test']
    kwargs:
      pg_num: 1024
- sleep:
    duration: 10
- ceph_manager.wait_for_clean: null
- sequential:
  - radosbench:
      pool: test
      size: 1
      time: 100
      cleanup: false
      create_pool: false
- install.upgrade:
    mon.a: null
- ceph.restart:
    daemons: [mon.a, mon.b, mon.c, osd.0, osd.1, osd.2]
    wait-for-healthy: false
    wait-for-osds-up: true
- exec:
    osd.0:
      - sleep 300 # http://tracker.ceph.com/issues/17808
      - ceph osd set require_jewel_osds
      - ceph osd set sortbitwise
- ceph.healthy:
- sleep:
    duration: 10
- ceph_manager.wait_for_clean: null
- exec:
    mon.a:
      - ceph osd set sortbitwise
- sleep:
    duration: 10
- ceph_manager.wait_for_clean: null
- sequential:
  - radosbench:
      pool: test
      size: 1
      time: 400
      cleanup: false
      create_pool: false
  - sleep:
      duration: 30
  - ceph_manager.kill_osd:
      kwargs:
        osd: 0
  - sleep:
      duration: 30
  - ceph_manager.revive_osd:
      kwargs:
        osd: 0
  - sleep:
      duration: 30
  - ceph_manager.wait_for_clean: null
- sleep:
    duration: 30
- ceph_manager.wait_for_clean: null
